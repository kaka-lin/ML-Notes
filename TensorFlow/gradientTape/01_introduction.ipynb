{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic Differentiation\n",
    "\n",
    "為了`自動微分(Automatic differentiation)`，TensorFlow 需要:\n",
    "\n",
    "1. `前向傳播(forward pass)`: 記住以什麼順序發生什麼樣的操作。\n",
    "2. `反向傳播(backward pass)`: 以相反的順序遍歷這個操作列表來計算梯度。\n",
    "\n",
    "在 TensorFlow 2 提供 [tf.GradientTape](https://www.tensorflow.org/api_docs/python/tf/GradientTape?hl=zh-tw) \n",
    "用於自動微分，也就是計算某些輸入的梯度 (Gradient)。\n",
    "\n",
    "Tensorflow 會將在 `tf.GradientTape` 上下文中執行的相關操作記錄到`\"磁帶(tape)\"`上。\n",
    "然後 tape 會計算反向傳播中的梯度。\n",
    "\n",
    "> TensorFlow \"records\" relevant operations executed inside the context of a tf.GradientTape onto a \"tape\". TensorFlow then uses that tape to compute the gradients of a \"recorded\" computation using [reverse mode differentiation](https://en.wikipedia.org/wiki/Automatic_differentiation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-31T04:20:12.743454Z",
     "start_time": "2022-08-31T04:20:12.670540Z"
    }
   },
   "source": [
    "## GradientTape\n",
    "\n",
    "[tf.GradientTape](https://www.tensorflow.org/api_docs/python/tf/GradientTape?hl=zh-tw) 默認將`所有可訓練的變量(tf.Variable, where trainable=True)`視為`需要監控的 node (watch_accessed_variables=True)`。API 如下:\n",
    "\n",
    "```python\n",
    "tf.GradientTape(\n",
    "    persistent=False, watch_accessed_variables=True\n",
    ")\n",
    "```\n",
    "\n",
    "> Record operations for automatic differentiation.\n",
    "\n",
    "- `persistent`: Boolean control whether a persistent gradient tape is created.\n",
    "- `watch_accessed_variables`: Boolean control whether the tape will automatically `watch` any (trainable) variables accessed while the tape is active."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-31T05:07:49.800071Z",
     "start_time": "2022-08-31T05:07:39.556531Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.9.1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing gradients\n",
    "\n",
    "用 [tf.GradientTape.gradient](https://www.tensorflow.org/api_docs/python/tf/GradientTape?hl=zh-tw#gradient) 來計算梯度，API 如下:\n",
    "\n",
    "```python\n",
    "tf.GradientTape.gradient(\n",
    "    target,\n",
    "    sources,\n",
    "    output_gradients=None,\n",
    "    unconnected_gradients=tf.UnconnectedGradients.NONE\n",
    ")\n",
    "```\n",
    "\n",
    "> Computes the gradient using operations recorded in context of this tape.\n",
    "\n",
    "- `target`: Tensor (or list of tensors) to be differentiated.\n",
    "- `sources`: a list or nested structure of Tensors or Variables. `target` will be differentiated against elements in `sources`.\n",
    "- `output_gradients`: a list of gradients, one for each element of target. Defaults to None.\n",
    "- `unconnected_gradients`: a value which can either hold 'none' or 'zero' and alters the value which will be returned if the target and sources are unconnected. The possible values and effects are detailed in 'UnconnectedGradients' and it defaults to 'none'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-31T05:03:10.880959Z",
     "start_time": "2022-08-31T05:03:10.834136Z"
    }
   },
   "source": [
    "For example, consider the function `y = x * x`. The gradient at `x = 3.0` can be computed as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-31T05:07:49.841885Z",
     "start_time": "2022-08-31T05:07:49.805818Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-31 13:07:49.809149: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.Variable(3.0) \n",
    "with tf.GradientTape() as tape:\n",
    "    y = x**2\n",
    "dy_dx = tape.gradient(y, x)\n",
    "dy_dx.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Controlling what the tape watches\n",
    "\n",
    "The default behavior is to record all operations after accessing a trainable `tf.Variable`. The reasons for this are:\n",
    "\n",
    "- The tape needs to know which operations to record in the forward pass to calculate the gradients in the backwards pass.\n",
    "- The tape holds references to intermediate outputs, so you don't want to record unnecessary operations.\n",
    "- The most common use case involves calculating the gradient of a loss with respect to all a model's trainable variables.\n",
    "\n",
    "Tape 默認的監控變數只有 `tf.Variable 且 trainable=True`，其他變數則會計算梯度失敗，如下:\n",
    "\n",
    "- `tf.Tensor`: not \"watched\"\n",
    "- `tf.Varaiable, trainble=False`\n",
    "\n",
    "對於以上不可訓練或沒有被監控的變量，可以使用 [tf.GradientTape.watch](https://www.tensorflow.org/api_docs/python/tf/GradientTape?hl=zh-tw#watch) 對其進行監控，API 如下:\n",
    "\n",
    "```python\n",
    "tf.GradientTape.watch(tensor)\n",
    "```\n",
    "\n",
    "> Ensures that tensor is being traced by this tape.\n",
    "\n",
    "- `tensor`: a Tensor or list of Tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-31T05:07:49.893504Z",
     "start_time": "2022-08-31T05:07:49.859024Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(6.0, shape=(), dtype=float32)\n",
      "None\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# A trainable variable\n",
    "x0 = tf.Variable(3.0, name='x0') # tf.Variable\n",
    "\n",
    "# Not trainable: `trainable=False`\n",
    "x1 = tf.Variable(3.0, name='x1', trainable=False) # tf.Variable\n",
    "\n",
    "# Not a variable: A variable + tensor returns a tensor.\n",
    "x2 = tf.Variable(2.0, name='x2') + 1.0 # tf.Tensor\n",
    "\n",
    "# Not a variable\n",
    "x3 = tf.constant(3.0, name='x3') # tf.Tensor\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    y = (x0**2) + (x1**2) + (x2**2) + (x3**2)\n",
    "\n",
    "grad = tape.gradient(y, [x0, x1, x2, x3])\n",
    "for g in grad:\n",
    "    print(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To record gradients with respect to a `tf.Tensor`, you need to call `GradientTape.watch(x)`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-31T05:07:49.923613Z",
     "start_time": "2022-08-31T05:07:49.903748Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(6.0, shape=(), dtype=float32)\n",
      "tf.Tensor(6.0, shape=(), dtype=float32)\n",
      "tf.Tensor(6.0, shape=(), dtype=float32)\n",
      "tf.Tensor(6.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x0 = tf.Variable(3.0, name='x0')\n",
    "x1 = tf.Variable(3.0, name='x1', trainable=False)\n",
    "x2 = tf.Variable(2.0, name='x2') + 1.0\n",
    "x3 = tf.constant(3.0, name='x3')\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch([x1, x2, x3])\n",
    "    y = (x0**2) + (x1**2) + (x2**2) + (x3**2)\n",
    "\n",
    "grad = tape.gradient(y, [x0, x1, x2, x3])\n",
    "for g in grad:\n",
    "    print(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Disable automatic tracking\n",
    "\n",
    "By default, GradientTape will automatically watch any trainable variables that are accessed inside the context.\n",
    "\n",
    "If you want `fine-grained control` over which variables are watched you disable automatic tracking by passing `watch_accessed_variables=False` to the tape constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-31T05:07:49.941798Z",
     "start_time": "2022-08-31T05:07:49.928584Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(6.0, shape=(), dtype=float32)\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "variable_a = tf.Variable(3.0, name='x1')\n",
    "variable_b = tf.Variable(2.0, name='x2')\n",
    "\n",
    "with tf.GradientTape(persistent=True, watch_accessed_variables=False) as disable_tracking_tape:\n",
    "    disable_tracking_tape.watch(variable_a)\n",
    "    y = variable_a ** 2 # Gradients will be available for `variable_a`.\n",
    "    z = variable_b ** 3 # No gradients will be available since `variable_b` is \n",
    "                        # not being watched.\n",
    "gradient_1 = disable_tracking_tape.gradient(y, variable_a) # 6.0\n",
    "gradient_2 = disable_tracking_tape.gradient(z, variable_b) # None\n",
    "\n",
    "print(gradient_1)\n",
    "print(gradient_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute multiple gradient\n",
    "\n",
    "By default, the resources held by a `GradientTape` are released as soon as `GradientTape.gradient()` method is called.\n",
    "\n",
    "To compute multiple gradients over the same computation, create `a persistent gradient tape`. This allows multiple calls to the gradient() method as resources are released when the tape object is garbage collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-31T05:07:49.961421Z",
     "start_time": "2022-08-31T05:07:49.945747Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First derivative of function y = x ^ 4 at x = 3 is 108.0\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable(3.0) \n",
    "with tf.GradientTape(persistent=True) as persistent_tape:\n",
    "    persistent_tape.watch(x)\n",
    "    y = x * x\n",
    "    z = y * y\n",
    "dz_dx = persistent_tape.gradient(z, x) # 108.0 (4*x^3 at x = 3)\n",
    "dy_dx = persistent_tape.gradient(y, x) # 6.0\n",
    "print(\"First derivative of function y = x ^ 4 at x = 3 is\", dz_dx.numpy())\n",
    "\n",
    "# Drop the reference to the tape\n",
    "del persistent_tape \n",
    "#persistent_tape # NameError: name 'persistent_tape' is not defined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nested Gradient\n",
    "\n",
    "GradientTapes can be nested to compute higher-order derivatives. For example,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-31T05:07:50.005211Z",
     "start_time": "2022-08-31T05:07:49.967795Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function: y = x * x, x = 3.0\n",
      "First Derivative: 6.0\n",
      "Second Derivative: 2.0\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable(3.0) \n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(x)\n",
    "    with tf.GradientTape() as tape2:\n",
    "        tape2.watch(x)\n",
    "        y = x * x\n",
    "    dy_dx = tape2.gradient(y, x)\n",
    "d2y_d2x = tape.gradient(dy_dx, x)\n",
    "\n",
    "print(\"Function: y = x * x, x = 3.0\")\n",
    "print(\"First Derivative:\", dy_dx.numpy())\n",
    "print(\"Second Derivative:\", d2y_d2x.numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "183px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "vscode": {
   "interpreter": {
    "hash": "ac2eaa0ea0ebeafcc7822e65e46aa9d4f966f30b695406963e145ea4a91cd4fc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
