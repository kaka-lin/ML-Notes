{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/kaka-lin/ML-Notes/blob/master/Model%20optimization/Post%20Training%20Quantization/Dynamic%20Range%20Quantization/example/dynamic_range_quantization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post Training Quantization - full integer quantization\n",
    "\n",
    "此範例使用`MNIST`，結果如下:\n",
    "\n",
    "```bash\n",
    "Model Size:\n",
    "{'baselin model (TF2)': 11149205,\n",
    " 'non quantized tflite': 11084920,\n",
    " 'ptq dynamic tflite': 2774512,\n",
    " 'ptq float fullback tflite': 2775032,\n",
    " 'ptq int only tflite': 2775056}\n",
    "Model Accuracy:\n",
    "{'baseline model': 0.9815,\n",
    " 'non quantized tflite': 0.9815,\n",
    " 'ptq dynamic tflite': 0.9815,\n",
    " 'ptq float fullback tflite': 0.9812,\n",
    " 'ptq int only tflite': 0.9814}\n",
    "```\n",
    "\n",
    "詳細步驟如下!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-02T09:49:12.456679Z",
     "start_time": "2019-10-02T09:49:01.312378Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ElDNGFeH5cZg",
    "outputId": "c88e12b7-6454-4722-f958-00b538ba8e79"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version:  2.9.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(depth=4)\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten, Conv2D\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"TensorFlow version: \", tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lez4wVqF5cZj"
   },
   "source": [
    "## Generate a TensorFlow Model\n",
    "\n",
    "包含 Load MNIST dataset, Build the model, Training and Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "定義會用到的相關函式，包含 `load data`, `build, train and test model`。\n",
    "如果不想看詳細可以直接跳至 [Starting Generate model\n",
    "](#Starting-Generate-model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     17
    ]
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    mnist_dataset = tf.keras.datasets.mnist.load_data()\n",
    "    (x_train, y_train), (x_test, y_test) = mnist_dataset\n",
    "    \n",
    "    # Normalize the input image so that each pixel value is between 0 to 1.\n",
    "    x_train = x_train.astype(np.float32) / 255.0\n",
    "    x_test = x_test.astype(np.float32) / 255.0\n",
    "\n",
    "    # Add a channels dimension\n",
    "    x_train = x_train[..., tf.newaxis]\n",
    "    x_test = x_test[..., tf.newaxis]\n",
    "    \n",
    "    return (x_train, y_train), (x_test, y_test)\n",
    "\n",
    "\n",
    "def calculate_tf2_model_size(model_name):\n",
    "    \"\"\"Calculate the model size of TensorFlow SaveModel format\"\"\"\n",
    "    size = 0\n",
    "    \n",
    "    # Tensorflow SaveModel format is a folder\n",
    "    for path, dirs, files in os.walk(model_name):\n",
    "        for f in files:\n",
    "            fp = os.path.join(path, f)\n",
    "            size += os.path.getsize(fp)\n",
    "    return size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0,
     19,
     35
    ]
   },
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        \n",
    "        # Define your layer here\n",
    "        self.conv1 = Conv2D(32, 3, activation='relu')\n",
    "        self.flatten = Flatten()\n",
    "        self.dense1 = Dense(128, activation='relu')\n",
    "        self.dense2 = Dense(10, activation='softmax')\n",
    "    \n",
    "    def call(self, x):\n",
    "        # Define your forward pass here\n",
    "        x = self.conv1(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dense2(x)\n",
    "        return x\n",
    "\n",
    "@tf.function\n",
    "def train_step(model, x_batch, y_batch, optimizer, loss_fn,\n",
    "               train_loss, train_accuracy):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(x_batch, training=True)\n",
    "        loss = loss_fn(y_batch, predictions)\n",
    "\n",
    "    # backward\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    \n",
    "    # Update training metric after batch\n",
    "    train_loss.update_state(loss)\n",
    "    train_accuracy.update_state(y_batch, predictions)\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def test_step(model, x_batch, y_batch, loss_fn,\n",
    "              test_loss, test_accuracy):\n",
    "    # forward\n",
    "    predictions = model(x_batch, training=False)\n",
    "    t_loss = loss_fn(y_batch, predictions)\n",
    "\n",
    "    test_loss.update_state(t_loss)\n",
    "    test_accuracy.update_state(y_batch, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the model\n",
    "\n",
    "注意: 這邊 optimizer 只能實例化一次，否則會有 error 如下:\n",
    "\n",
    "```bash\n",
    "ValueError: tf.function only supports singleton tf.Variables created on the first call. Make sure the tf.Variable is only created once or created outside tf.function. See https://www.tensorflow.org/guide/function#creating_tfvariables for more information.\n",
    "```\n",
    "\n",
    "參考: [Using with multiple Keras optimizer](https://www.tensorflow.org/guide/function#using_with_multiple_keras_optimizers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-02T09:49:13.017410Z",
     "start_time": "2019-10-02T09:49:12.458793Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aQAZ7ZcI5cZl",
    "outputId": "2cd5f8a0-617d-4759-9e40-7acdfe444514",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = load_data()\n",
    "train_images, train_labels = x_train, y_train\n",
    "test_images, test_labels = x_test, y_test\n",
    "\n",
    "# Preprocessing dataset\n",
    "# Use `tf.data`` to batch and shuffle the dataset:\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (x_train, y_train)).shuffle(10000).batch(32)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (x_test, y_test)).batch(32)\n",
    "\n",
    "# Build the model\n",
    "model = MyModel()\n",
    "\n",
    "# If we want to using model.summary()\n",
    "model.build(input_shape=(None, 28, 28, 1))\n",
    "model.call(Input(shape=(28, 28, 1)))\n",
    "# model.summary()\n",
    "\n",
    "# Compile the model: optimizer and loss\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')\n",
    "\n",
    "# 建立評估模型的dict\n",
    "MODEL_SIZE = {}\n",
    "MODEL_ACCURACY = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting Generate model\n",
    "\n",
    "This training won't take long because you're training the model for just a 3 epochs, which trains to about ~98% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     100%|██████████████████████████████| 1875/1875 [00:05<00:00, 351.98it/s, loss=0.134, accuracy=0.96] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     100%|██████████████████████████████| 1875/1875 [00:03<00:00, 478.04it/s, loss=0.0434, accuracy=0.987]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     100%|██████████████████████████████| 1875/1875 [00:03<00:00, 478.02it/s, loss=0.0218, accuracy=0.993]\n",
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_data/baseline_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_data/baseline_model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     100%|██████████████████████████████| 313/313 [00:00<00:00, 464.71it/s, loss=0.0618, accuracy=0.982]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.9815000295639038\n",
      "Model Size:\n",
      "{'baselin model (TF2)': 11149205}\n",
      "Model Accuracy:\n",
      "{'baseline model': 0.9815}\n"
     ]
    }
   ],
   "source": [
    "#########################################################################################################\n",
    "# Strating training\n",
    "EPOCHS = 3\n",
    "n_batches = len(train_dataset)\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f'Epoch {epoch+1}/{EPOCHS}')\n",
    "    with tqdm(train_dataset, total=n_batches,\n",
    "              bar_format='{desc:<5.5}{percentage:3.0f}%|{bar:30}{r_bar}') as pbar:\n",
    "        for batch, (x_train, y_train) in enumerate(pbar):\n",
    "            train_step(model, x_train, y_train, optimizer, loss_fn,\n",
    "                       train_loss, train_accuracy)\n",
    "            pbar.set_postfix({\n",
    "                'loss': train_loss.result().numpy(),\n",
    "                'accuracy': train_accuracy.result().numpy()})\n",
    "\n",
    "    # Reset the metrics at the start of the next epoch\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "\n",
    "# save model\n",
    "model.save('model_data/baseline_model', include_optimizer=False)\n",
    "\n",
    "#########################################################################################################\n",
    "# Testing\n",
    "n_batches = len(test_dataset)\n",
    "print(f'Testing:')\n",
    "with tqdm(test_dataset, total=n_batches,\n",
    "          bar_format='{desc:<5.5}{percentage:3.0f}%|{bar:30}{r_bar}') as pbar:\n",
    "    for batch, (x_test, y_test) in enumerate(pbar):\n",
    "        test_step(model, x_test, y_test, loss_fn, test_loss, test_accuracy)\n",
    "        pbar.set_postfix({\n",
    "            'loss': test_loss.result().numpy(),\n",
    "            'accuracy': test_accuracy.result().numpy()})\n",
    "    \n",
    "test_acc = test_accuracy.result().numpy()\n",
    "    \n",
    "# Reset the metrics at the start of the next epoch\n",
    "test_loss.reset_states()\n",
    "test_accuracy.reset_states()\n",
    "print(f\"Test accuracy: {test_acc}\")\n",
    "\n",
    "#########################################################################################################\n",
    "# Record model size and accuracy\n",
    "MODEL_SIZE['baselin model (TF2)'] = calculate_tf2_model_size('model_data/baseline_model') # / 1000000 MB\n",
    "MODEL_ACCURACY['baseline model'] = test_acc\n",
    "\n",
    "print(\"Model Size:\")\n",
    "pp.pprint(MODEL_SIZE)\n",
    "print(\"Model Accuracy:\")\n",
    "pp.pprint(MODEL_ACCURACY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to a TFLite model\n",
    "\n",
    "請注意，某些版本的量化會將某些數據保留為浮點格式。 因此，以下部分將顯示每個選項的量化量增加，直到我們得到一個完全由 `int8` 或 `uint8` 數據組成的模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. No quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Convert the model\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model('model_data/baseline_model')                                          \n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the model.\n",
    "with open('model_data/model_no_quant.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's now a TensorFlow Lite model, but it's still using 32-bit float values for all parameter data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Dynamic range quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Convert the model: Dynamic range quantization\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model('model_data/baseline_model')\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "tflite_quant_model = converter.convert()\n",
    "\n",
    "# Save the model.\n",
    "with open('model_data/model_ptq_dynamic.tflite', 'wb') as f:\n",
    "    f.write(tflite_quant_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is now a bit smaller with quantized weights, but other variable data is still in float format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Integer with float fallback quantization (using default float input/output)\n",
    "\n",
    "To quantize the variable data (such as model `input/output` and `intermediates between layers`), you need to provide a `RepresentativeDataset`. \n",
    "\n",
    "###### RepresentativeDataset\n",
    "\n",
    "This is a generator function that provides a set of input data that's large enough to represent typical values. It allows the converter to estimate a dynamic range for all the variable data. (The dataset does not need to be unique compared to the training or evaluation dataset.) To support multiple inputs, each representative data point is a list and elements in the list are fed to the model according to their indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fully_quantize: 0, inference_type: 6, input_inference_type: 0, output_inference_type: 0\n"
     ]
    }
   ],
   "source": [
    "def representative_data_gen():    \n",
    "    for input_value in tf.data.Dataset.from_tensor_slices(train_images).batch(1).take(100):\n",
    "        # Model has only one input so each data point has one element.\n",
    "        yield [input_value]\n",
    "\n",
    "# Convert the model: Integer with float fallback quantization\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model('model_data/baseline_model')\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_data_gen\n",
    "tflite_quant_model = converter.convert()\n",
    "\n",
    "# Save the model.\n",
    "with open('model_data/model_ptq_int_float_fullback.tflite', 'wb') as f:\n",
    "    f.write(tflite_quant_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all weights and variable data are quantized, and the model is significantly smaller compared to the original TensorFlow Lite model.\n",
    "\n",
    "However, to maintain compatibility with applications that traditionally use float model input and output tensors, the TensorFlow Lite Converter leaves the model input and output tensors in float:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:  <class 'numpy.float32'>\n",
      "output:  <class 'numpy.float32'>\n"
     ]
    }
   ],
   "source": [
    "# Load the model into the interpreter\n",
    "interpreter = tf.lite.Interpreter(model_content=tflite_quant_model)\n",
    "input_type = interpreter.get_input_details()[0]['dtype']\n",
    "print('input: ', input_type)\n",
    "output_type = interpreter.get_output_details()[0]['dtype']\n",
    "print('output: ', output_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此模式通常有利於兼容性，但它與僅執行 integer-baesd 的操作設備不兼容，例如 `Edge TPU`。\n",
    "\n",
    "Additionally, the above process may leave an operation in float format if TensorFlow Lite doesn't include a quantized implementation for that operation. This strategy allows conversion to complete so you have a smaller and more efficient model, but again, it won't be compatible with integer-only hardware. (All ops in this MNIST model have a quantized implementation.)\n",
    "\n",
    "So to ensure an end-to-end integer-only model, you need a couple more parameters..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Integer-only quantization\n",
    "\n",
    "To quantize the input and output tensors, and make the converter throw an error if it encounters an operation it cannot quantize, convert the model again with some additional parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fully_quantize: 0, inference_type: 6, input_inference_type: 3, output_inference_type: 3\n"
     ]
    }
   ],
   "source": [
    "def representative_data_gen():    \n",
    "    for input_value in tf.data.Dataset.from_tensor_slices(train_images).batch(1).take(100):\n",
    "        # Model has only one input so each data point has one element.\n",
    "        yield [input_value]\n",
    "\n",
    "# Convert the model: Integer-only quantization\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model('model_data/baseline_model')\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_data_gen\n",
    "# Ensure that if any ops can't be quantized, the converter throws an error\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "# Set the input and output tensors to uint8 (APIs added in r2.3)\n",
    "converter.inference_input_type = tf.uint8  # or tf.int8\n",
    "converter.inference_output_type = tf.uint8  # or tf.int8\n",
    "tflite_quant_model = converter.convert()\n",
    "\n",
    "# Save the model.\n",
    "with open('model_data/model_ptq_int_only.tflite', 'wb') as f:\n",
    "    f.write(tflite_quant_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The internal quantization remains the same as above, but you can see the input and output tensors are now integer format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:  <class 'numpy.uint8'>\n",
      "output:  <class 'numpy.uint8'>\n"
     ]
    }
   ],
   "source": [
    "# Load the model into the interpreter\n",
    "interpreter = tf.lite.Interpreter(model_content=tflite_quant_model)\n",
    "input_type = interpreter.get_input_details()[0]['dtype']\n",
    "print('input: ', input_type)\n",
    "output_type = interpreter.get_output_details()[0]['dtype']\n",
    "print('output: ', output_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you have an integer quantized model that uses integer data for the model's input and output tensors, so it's compatible with integer-only hardware such as the Edge TPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the TFLite model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function for running tflite model\n",
    "\n",
    "先建立 TFLite 的評估模型準確率的函式。參考[官方範例](https://www.tensorflow.org/lite/performance/post_training_integer_quant_16x8#evaluate_the_models)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Helper function to run inference on a TFLite model\n",
    "def evaluate_model(tflite_file):\n",
    "    # Initialize the interpreter\n",
    "    interpreter = tf.lite.Interpreter(model_path=str(tflite_file))\n",
    "    interpreter.allocate_tensors()\n",
    "    \n",
    "    input_details = interpreter.get_input_details()[0]\n",
    "    output_details = interpreter.get_output_details()[0]\n",
    "    \n",
    "    # Run predictions on every image in the \"test\" dataset.\n",
    "    prediction_digits = []\n",
    "    for test_image in test_images:\n",
    "        \n",
    "        # Check if the input type is quantized, then rescale input data to uint8\n",
    "        if input_details['dtype'] == np.uint8:\n",
    "            input_scale, input_zero_point = input_details[\"quantization\"]\n",
    "            test_image = test_image / input_scale + input_zero_point\n",
    "\n",
    "        # Pre-processing: add batch dimension and convert to float32 to match with\n",
    "        # the model's input data format.\n",
    "        test_image = np.expand_dims(test_image, axis=0).astype(input_details[\"dtype\"])\n",
    "        interpreter.set_tensor(input_details[\"index\"], test_image)\n",
    "        \n",
    "        # Run inference.\n",
    "        interpreter.invoke()\n",
    "        \n",
    "        # Post-processing: remove batch dimension and find the digit with highest\n",
    "        # probability.\n",
    "        output = interpreter.tensor(output_details[\"index\"])\n",
    "        digit = np.argmax(output()[0])\n",
    "        prediction_digits.append(digit)\n",
    "    \n",
    "    # Compare prediction results with ground truth labels to calculate accuracy.\n",
    "    accurate_count = 0\n",
    "    for index in range(len(prediction_digits)):\n",
    "        if prediction_digits[index] == test_labels[index]:\n",
    "            accurate_count += 1\n",
    "    accuracy = accurate_count * 1.0 / len(prediction_digits)\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    }
   ],
   "source": [
    "# evaluate the TF Lite model\n",
    "no_quant_acc = evaluate_model('model_data/model_no_quant.tflite')\n",
    "ptq_dynamic_acc = evaluate_model('model_data/model_ptq_dynamic.tflite')\n",
    "ptq_int_float_fullback_acc = evaluate_model('model_data/model_ptq_int_float_fullback.tflite')\n",
    "ptq_int_only_acc = evaluate_model('model_data/model_ptq_int_only.tflite')\n",
    "\n",
    "# Record model size and accuracy\n",
    "MODEL_SIZE['non quantized tflite'] = os.path.getsize('model_data/model_no_quant.tflite') # / 1000000 MB\n",
    "MODEL_ACCURACY['non quantized tflite'] = no_quant_acc\n",
    "\n",
    "MODEL_SIZE['ptq dynamic tflite'] = os.path.getsize('model_data/model_ptq_dynamic.tflite') # / 1000000 MB\n",
    "MODEL_ACCURACY['ptq dynamic tflite'] = ptq_dynamic_acc\n",
    "\n",
    "MODEL_SIZE['ptq float fullback tflite'] = os.path.getsize('model_data/model_ptq_int_float_fullback.tflite') # / 1000000 MB\n",
    "MODEL_ACCURACY['ptq float fullback tflite'] = ptq_int_float_fullback_acc\n",
    "\n",
    "MODEL_SIZE['ptq int only tflite'] = os.path.getsize('model_data/model_ptq_int_only.tflite') # / 1000000 MB\n",
    "MODEL_ACCURACY['ptq int only tflite'] = ptq_int_only_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Size:\n",
      "{'baselin model (TF2)': 11149205,\n",
      " 'non quantized tflite': 11084920,\n",
      " 'ptq dynamic tflite': 2774512,\n",
      " 'ptq float fullback tflite': 2775032,\n",
      " 'ptq int only tflite': 2775056}\n",
      "Model Accuracy:\n",
      "{'baseline model': 0.9815,\n",
      " 'non quantized tflite': 0.9815,\n",
      " 'ptq dynamic tflite': 0.9815,\n",
      " 'ptq float fullback tflite': 0.9812,\n",
      " 'ptq int only tflite': 0.9814}\n"
     ]
    }
   ],
   "source": [
    "print(\"Model Size:\")\n",
    "pp.pprint(MODEL_SIZE)\n",
    "print(\"Model Accuracy:\")\n",
    "pp.pprint(MODEL_ACCURACY)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "338.188px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "f4ddee0af85aea83384ab50670e5d73ce218e0f8fdea7864014776ee088b96fe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
